
R version 2.4.0 (2006-10-03)
Copyright (C) 2006 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> invisible(options(echo = TRUE))
> nova = read.table("scaled.csv", sep=",")
> dim(nova)
[1] 10000    20
> nova$V1 = as.factor(nova$V1)
> 
> #Split into training and test sets
> set.seed(19848492)
> test.samp = c(sample(5000, size = 500), 5000 + sample(5000, size = 500))
> nova.train = nova[-test.samp,]
> nova.test = nova[test.samp,]
> library(e1071)
Loading required package: class
> 
> #Set up cross validation sets
> #set.seed(11039348)
> #cross.val = matrix(sample(9000), ncol = 10)
> 
> costs = c(0.5,(1:10)^2)
> gammas =  0.05 * (1:10)^2
> obj <- tune(svm, V1 ~ ., data = nova.train, 
+               ranges = list(gamma = gammas, cost = costs),
+              )
> obj$performances
    gamma  cost      error
1    0.05   0.5 0.04390336
2    0.20   0.5 0.04568427
3    0.45   0.5 0.06412502
4    0.80   0.5 0.10435345
5    1.25   0.5 0.16469329
6    1.80   0.5 0.23480411
7    2.45   0.5 0.31860107
8    3.20   0.5 0.39760105
9    4.05   0.5 0.45248916
10   5.00   0.5 0.47966844
11   0.05   1.0 0.04145652
12   0.20   1.0 0.04512808
13   0.45   1.0 0.05412574
14   0.80   1.0 0.08335053
15   1.25   1.0 0.12602479
16   1.80   1.0 0.17280701
17   2.45   1.0 0.22002486
18   3.20   1.0 0.26370797
19   4.05   1.0 0.30015460
20   5.00   1.0 0.32839080
21   0.05   4.0 0.04034129
22   0.20   4.0 0.04723363
23   0.45   4.0 0.05379613
24   0.80   4.0 0.07824044
25   1.25   4.0 0.11735564
26   1.80   4.0 0.15680912
27   2.45   4.0 0.20280088
28   3.20   4.0 0.24236971
29   4.05   4.0 0.27804675
30   5.00   4.0 0.30660300
31   0.05   9.0 0.04033942
32   0.20   9.0 0.04934639
33   0.45   9.0 0.05568331
34   0.80   9.0 0.07868403
35   1.25   9.0 0.11735564
36   1.80   9.0 0.15680912
37   2.45   9.0 0.20280088
38   3.20   9.0 0.24236971
39   4.05   9.0 0.27804675
40   5.00   9.0 0.30660300
41   0.05  16.0 0.04167191
42   0.20  16.0 0.05290176
43   0.45  16.0 0.05601838
44   0.80  16.0 0.07868403
45   1.25  16.0 0.11735564
46   1.80  16.0 0.15680912
47   2.45  16.0 0.20280088
48   3.20  16.0 0.24236971
49   4.05  16.0 0.27804675
50   5.00  16.0 0.30660300
51   0.05  25.0 0.04366982
52   0.20  25.0 0.05356967
53   0.45  25.0 0.05624135
54   0.80  25.0 0.07868403
55   1.25  25.0 0.11735564
56   1.80  25.0 0.15680912
57   2.45  25.0 0.20280088
58   3.20  25.0 0.24236971
59   4.05  25.0 0.27804675
60   5.00  25.0 0.30660300
61   0.05  36.0 0.04633964
62   0.20  36.0 0.05501215
63   0.45  36.0 0.05624135
64   0.80  36.0 0.07868403
65   1.25  36.0 0.11735564
66   1.80  36.0 0.15680912
67   2.45  36.0 0.20280088
68   3.20  36.0 0.24236971
69   4.05  36.0 0.27804675
70   5.00  36.0 0.30660300
71   0.05  49.0 0.04900672
72   0.20  49.0 0.05500768
73   0.45  49.0 0.05624135
74   0.80  49.0 0.07868403
75   1.25  49.0 0.11735564
76   1.80  49.0 0.15680912
77   2.45  49.0 0.20280088
78   3.20  49.0 0.24236971
79   4.05  49.0 0.27804675
80   5.00  49.0 0.30660300
81   0.05  64.0 0.05022611
82   0.20  64.0 0.05534264
83   0.45  64.0 0.05624135
84   0.80  64.0 0.07868403
85   1.25  64.0 0.11735564
86   1.80  64.0 0.15680912
87   2.45  64.0 0.20280088
88   3.20  64.0 0.24236971
89   4.05  64.0 0.27804675
90   5.00  64.0 0.30660300
91   0.05  81.0 0.05078204
92   0.20  81.0 0.05534363
93   0.45  81.0 0.05624135
94   0.80  81.0 0.07868403
95   1.25  81.0 0.11735564
96   1.80  81.0 0.15680912
97   2.45  81.0 0.20280088
98   3.20  81.0 0.24236971
99   4.05  81.0 0.27804675
100  5.00  81.0 0.30660300
101  0.05 100.0 0.05189441
102  0.20 100.0 0.05590143
103  0.45 100.0 0.05624135
104  0.80 100.0 0.07868403
105  1.25 100.0 0.11735564
106  1.80 100.0 0.15680912
107  2.45 100.0 0.20280088
108  3.20 100.0 0.24236971
109  4.05 100.0 0.27804675
110  5.00 100.0 0.30660300
> obj$best.model

Call:
best.tune(method = svm, train.x = V1 ~ ., data = nova.train, ranges = list(gamma = gammas, 
    cost = costs))


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  9 
      gamma:  0.05 

Number of Support Vectors:  1202

> 
> #Performance of best model on test set.
> table(nova.test$V1, predict(obj$best.model, nova.test))
   
      0   1
  0 484  16
  1  32 468
> proc.time()
[1] 104827.795     92.681 104937.775      0.000      0.000
> 
